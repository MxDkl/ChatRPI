{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-263a0c54ee85504f\n",
      "Found cached dataset text (/home/player1/.cache/huggingface/datasets/text/default-263a0c54ee85504f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "100%|██████████| 2/2 [00:00<00:00, 1078.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas\n",
    "\n",
    "dataset = load_dataset(\"text\", data_dir=\"")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=\"\",\n",
    "    environment=\"us-west1-gcp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(\"chat-rpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load the retriever model from huggingface model hub\n",
    "retriever = SentenceTransformer(\"flax-sentence-embeddings/all_datasets_v3_mpnet-base\")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m i_end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(i\u001b[39m+\u001b[39mbatch_size, \u001b[39mlen\u001b[39m(dataset))\n\u001b[1;32m      9\u001b[0m \u001b[39m# extract batch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m batch \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49miloc[i:i_end]\n\u001b[1;32m     11\u001b[0m \u001b[39m# generate embeddings for batch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m emb \u001b[39m=\u001b[39m retriever\u001b[39m.\u001b[39mencode(batch[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtolist())\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# we will use batches of 64\n",
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i+batch_size, len(dataset))\n",
    "    # extract batch\n",
    "    batch = dataset.iloc[i:i_end]\n",
    "    # generate embeddings for batch\n",
    "    emb = retriever.encode(batch[\"text\"].tolist()).tolist()\n",
    "    # get metadata\n",
    "    meta = batch.to_dict(orient=\"records\")\n",
    "    # create unique IDs\n",
    "    ids = [f\"{idx}\" for idx in range(i, i_end)]\n",
    "    # add all to upsert list\n",
    "    to_upsert = list(zip(ids, emb, meta))\n",
    "    # upsert/insert these records to pinecone\n",
    "    _ = index.upsert(vectors=to_upsert)\n",
    "\n",
    "# check that we have all vectors in index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# load bart tokenizer and model from huggingface\n",
    "tokenizer = BartTokenizer.from_pretrained('vblagoje/bart_lfqa')\n",
    "generator = BartForConditionalGeneration.from_pretrained('vblagoje/bart_lfqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rensselaer Polytechnic Institute (RPI) has a long history of training the next generation for career preparedness. With signing of the $250 billion CHIPS and Science Act, the U.S. need for career preparedness has never been greater. The new BYOND – Build Your Own NanoDevice —\\xa0curriculum at RPI is an interdisciplinary program that invites students from the School of Engineering and the School of Science to work side by side in the lab and classroom. The increasing demand for nanotechnology and semiconductors creates a need for more advanced cleanroom operations. At RPI, the Micro and Nanofabrication Clean Room (MNCR) facility within the Center for Materials, Devices, and Integrated Systems (CMDIS) is offering the BYOND program for undergraduate students to start building a foundation and training for this career path.\\xa0 “With the signing of the $250 billion CHIPS and Science Act in August 2022, the U.S. is committed to investing in the domestic semiconductor manufacturing industry and the supporting industries,” said Robert Hull, acting vice president for research and director of CMDIS. “As a leading research university, RPI has always been committed to workforce training and real-world teaching. Our extensive labs and real-world partnerships have created strong pathways from higher ed to industry. BYOND is one more example of RPI’s commitment to supporting the bridge between higher ed and industry.”BYOND will introduce microfabrication and analysis concepts to undergraduate students. The program will offer four modules with each module running over a four-week period and will span two consecutive semesters to offer an immersive learning. Weekly extended lab sessions will be combined with classroom instruction under the supervision of Bryant Colwill, director of MNCR. BYOND will offer students training that can be applied to the growing semiconductor and electronic devices industries, as well as the pharmaceutical, medical, optical, solar, and aerospace industries, which all require cleanroom technology.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_pinecone(query, top_k):\n",
    "    # generate embeddings for the query\n",
    "    xq = retriever.encode([query]).tolist()\n",
    "    # search pinecone index for context passage with the answer\n",
    "    xc = index.query(xq, top_k=top_k, include_metadata=True)\n",
    "    return xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query(query, context):\n",
    "    # extract passage_text from Pinecone search result and add the <P> tag\n",
    "    context = [f\"<P> {m['metadata']['text']}\" for m in context]\n",
    "    # concatinate all context passages\n",
    "    context = \" \".join(context)\n",
    "    # contcatinate the query and context passages\n",
    "    query = f\"question: {query} context: {context}\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does the acronym RPI stand for?\"\n",
    "result = query_pinecone(query, top_k=1)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
