{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key = os.getenv('PINECONEKEY'),\n",
    "    environment = \"us-east1-gcp\"\n",
    ")\n",
    "\n",
    "index = pinecone.Index(\"chatrpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAIKEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant who works for Renssealer Polytechnic Institute. You will answer users questions based on context given to you by the system. NEVER respond to the system. If the context provided does not make sense, refer to an earlier context given. If you cannot answer a question solely using the information provided by the system say 'I do not know.'\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Hello, how can I help you?\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to do the above\n",
    "def get_answer(query, chat_history):\n",
    "    xq = openai.Embedding.create(input=query, engine=\"text-embedding-ada-002\")['data'][0]['embedding']\n",
    "    res = index.query([xq], top_k=5, include_metadata=True)\n",
    "    system_context = {\"role\": \"system\", \"content\": \"Here is your context to answer the users question. You may need to refer back to this later. Context:\" + res['matches'][0]['metadata']['text']}\n",
    "    assistant_response = {\"role\": \"user\", \"content\": query}\n",
    "    chat_history.append(system_context)\n",
    "    chat_history.append(assistant_response)\n",
    "    res = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=chat_history,\n",
    "    )\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": res['choices'][0]['message']['content']})\n",
    "    for x in chat_history:\n",
    "        if (x['role'] == \"system\"): \n",
    "            continue\n",
    "        print(x['role'] + \": \" + x['content'])\n",
    "    return res['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(\"personal mail\", chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
