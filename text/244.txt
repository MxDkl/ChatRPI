Computer Science Colloquia & Seminars are held each semester and sponsored by the Computer Science department. Faculty invite speakers from all areas of computer science, and the talks are open to all members of the RPI community.   Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural NetworkLorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC NetworksJesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANsShawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2GeneWilliam HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode DecompositionNeha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-IdentificationIan ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations AnalysisRuixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed FusionAshley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and VisualizationOhad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin MonkeysConnor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity AlgorithmsRoman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph ProblemsMohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary wordArtificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines). In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems. Bio:Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equallysuccessful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills. Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.githubToday connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function. Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran. Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations). In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework. Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem.  Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society. Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.References:[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022) Bio:Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.Muhammad Saad AtiqueTaming Uncertainty in Social NetworksAdvisor: Prof. Bolek Szymanski Sahith BhamidipatiGenerating Synthetic Election DataAdvisor: Prof. Lirong Xia Matthew CrottyDetermining Image Hardness using Ensemble Classifier MethodAdvisor: Prof. Radoslav Ivanov Olivia LundeliusAn Analysis of Improved Daltonization AlgorithmsAdvisor: Prof. Barb Cutler Nicholas LutrzykowskiDriver for Seizure Prediction Using EEG DataAdvisor: Prof. Bulent Yener Richard PawelkiewiczAction at a DistanceAdvisor: Prof. Bulent Yener Noah PrisamentA Variation on the Hotelling-Downs Model with Facility Synergy: the Mall EffectAdvisor: Prof. Elliot Anshelevich Jeff PutlockOvercoming Missing Data and Labels During VFLAdvisor: Prof. Stacy Patterson Vijay SadashivaiahTowards Explainable Transfer LearningAdvisor: Prof. Jim Hendler Bishwajit SahaClustering with Associative MemoriesAdvisor: Prof. Mohammed Zaki Mara SchwartzTopical Analysis of Twitter User ClustersAdvisor: Prof. Tomek Strzalkowski Xiao ShouEvent Former: A Self-Supervised Learning Paradigm for Temporal Point ProcessAdvisor: Prof. Kristin BennettThe presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis. Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application.  Speaker Bio:Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.Presenters will be:Aidan LaneFuzzy Search on Encrypted DataAdvisor: Konstanin KuzminLeon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel AdaliCaitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian CallahanNetworks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market.  Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI. Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research". Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.  Bio:Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.Quantum can solve complex problems that classical computers will never be able to. In recent years, significant efforts have been devoted to building quantum computers to solve real-world problems. To ensure the correctness of quantum systems, we develop the verification techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview my work of efficient reasoning about quantum programs by developing verification techniques and tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will review my work on quantum state tomography, i.e., learning the classical description of quantum hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering. Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  the University of Technology Sydney. He received his B.S. and PhD degrees from the Department of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design. Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses. Bio:Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information207 Lally Hall110 8th StreetTroy, NY 12180-3590(518) 276 8412
            Copyright © 2019 Rensselaer Polytechnic Institute (RPI)
          ProgramsResearchFacultyNewsEventsContactEventsComputer Science Colloquium
Competitions and Hackathons
PhD Thesis Defenses
Computer Science ColloquiumCompetitions and HackathonsPhD Thesis Defenses











☰





  
 

School of Science
Computer Science
  





Programs
Research
Faculty
News
Events
Contact





Colloquia, Seminars, & Graduate Student Poster Sessions



Computer Science Colloquia & Seminars are held each semester and sponsored by the Computer Science department. Faculty invite speakers from all areas of computer science, and the talks are open to all members of the RPI community. 
 
 





2023

 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 
  
   

 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 
  
   

 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 
  
   

 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 
  
   

 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 
  
   

 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 
  
   

 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 
  
   

 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 
  
   

 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 
  
   

 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 
  
   

 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 
  
   

 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 
  
   
2022

 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 
  
   

 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 
  
   

 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 
  
   

 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 
  
   

 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 
  
   

 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 
  
   

 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
15
2022







 
  
   

 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   

 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 
  
   

 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   










EventsComputer Science Colloquium
Competitions and Hackathons
PhD Thesis Defenses







 Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
  














Rensselaer Polytechnic Institute


              110 Eighth Street |
              Troy, NY USA 12180
            

(518) 276-6000

Contact Us







Policies:

      Media

      Web Privacy

      Title IX


    Student Consumer Information

    Accessibility
 
            Copyright © 2019 Rensselaer Polytechnic Institute (RPI)
          


































☰





  
 

School of Science
Computer Science
  





Programs
Research
Faculty
News
Events
Contact










☰
☰☰



  
 

School of Science
Computer Science
  





  
 

School of Science
Computer Science
  



  
 

School of Science
Computer Science
  

  
 

School of Science
Computer Science
     

School of Science
Computer Science
 

School of Science
Computer Science

School of Science
Computer ScienceSchool of Science

Programs
Research
Faculty
News
Events
Contact


Programs
Research
Faculty
News
Events
Contact

Colloquia, Seminars, & Graduate Student Poster Sessions



Computer Science Colloquia & Seminars are held each semester and sponsored by the Computer Science department. Faculty invite speakers from all areas of computer science, and the talks are open to all members of the RPI community. 
 
 





2023

 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 
  
   

 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 
  
   

 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 
  
   

 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 
  
   

 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 
  
   

 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 
  
   

 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 
  
   

 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 
  
   

 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 
  
   

 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 
  
   

 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 
  
   

 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 
  
   
2022

 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 
  
   

 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 
  
   

 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 
  
   

 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 
  
   

 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 
  
   

 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 
  
   

 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
15
2022







 
  
   

 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   

 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 
  
   

 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   










EventsComputer Science Colloquium
Competitions and Hackathons
PhD Thesis Defenses







 Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
  








Computer Science Colloquia & Seminars are held each semester and sponsored by the Computer Science department. Faculty invite speakers from all areas of computer science, and the talks are open to all members of the RPI community. 
 
 





2023

 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 
  
   

 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 
  
   

 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 
  
   

 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 
  
   

 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 
  
   

 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 
  
   

 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 
  
   

 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 
  
   

 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 
  
   

 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 
  
   

 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 
  
   

 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 
  
   
2022

 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 
  
   

 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 
  
   

 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 
  
   

 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 
  
   

 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 
  
   

 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 
  
   

 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
15
2022







 
  
   

 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   

 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 
  
   

 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   



Computer Science Colloquia & Seminars are held each semester and sponsored by the Computer Science department. Faculty invite speakers from all areas of computer science, and the talks are open to all members of the RPI community. 
 
 
Computer Science Colloquia & Seminars are held each semester and sponsored by the Computer Science department. Faculty invite speakers from all areas of computer science, and the talks are open to all members of the RPI community. 
 
 
Computer Science Colloquia & Seminars are held each semester and sponsored by the Computer Science department. Faculty invite speakers from all areas of computer science, and the talks are open to all members of the RPI community. 
 
 



2023

 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 
  
   

 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 
  
   

 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 
  
   

 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 
  
   

 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 
  
   

 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 
  
   

 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 
  
   

 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 
  
   

 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 
  
   

 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 
  
   

 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 
  
   

 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 
  
   
2022

 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 
  
   

 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 
  
   

 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 
  
   

 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 
  
   

 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 
  
   

 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 
  
   

 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
15
2022







 
  
   

 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   

 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 
  
   

 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   




2023

 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 
  
   

 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 
  
   

 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 
  
   

 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 
  
   

 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 
  
   

 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 
  
   

 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 
  
   

 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 
  
   

 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 
  
   

 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 
  
   

 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 
  
   

 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 
  
   
2022

 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 
  
   

 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 
  
   

 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 
  
   

 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 
  
   

 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 
  
   

 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 
  
   

 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
15
2022







 
  
   

 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   

 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 
  
   

 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   


2023

 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 
  
   

 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 
  
   

 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 
  
   

 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 
  
   

 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 
  
   

 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 
  
   

 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 
  
   

 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 
  
   

 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
   

 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 
  
   

 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 
  
   

 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 
  
   

 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 
  
   
2022

 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 
  
   

 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 
  
   

 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 
  
   

 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 
  
   

 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 
  
   

 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 
  
   

 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
   

 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Mar
15
2022







 
  
   

 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   

 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 
  
   

 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
   

 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
   

 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 
  
    

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


 

Apr
7
2023




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


Apr
7
2023
Apr72023


Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word



Computer Science Graduate Students
Lally 104   4:00 pm


Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word




Computer Science Poster Session
Vasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word

Computer Science Poster SessionVasundhara AcharyaAdvisor: Prof. Bulent YenerTitle: Tuberculosis Prediction from Lung Tissue Images of Diversity Outbred Mice using Cell Graph Neural Network
Lorson BlairAdvisor: Prof. Stacy PattersonTitle: A Continuum Approach for Collaborative Task Processing in UAV MEC Networks
Jesse EllinAdvisor: Prof. Alex GittensTitle: Knowledge Graph Anomaly Detection via Probabilistic GANs
Shawn GeorgeAdvisor: Prof. Konstantin KuzminTitle: Synergy: Abstract2Gene
William HawkinsAdvisor: Prof. George SlotaTitle: Accelerating Graph Neural Network Training using Dynamic Mode Decomposition
Neha DeshpandeAdvisor: Prof. Chuck StewartTitle: Tusk Detection for Elephant Re-Identification
Ian ConradAdvisor: Prof. Sibel AdaliTitle: Contextualized Moral Foundations Analysis
Ruixiong HuAdvisor: Mark ShephardTitle: Mesh Adaptation in Multilayer Laser Powder Bed Fusion
Ashley ChoiAdvisor: Prof. Sibel AdaliTitle: News Story Collection API and Visualization
Ohad NirAdvisor: Prof. Chuck StewartTitle: Detection of Capuchin Monkeys
Connor WoodingAdvisor: Prof. George SlotaTitle: GPU Parallelization for Biconnectivity Algorithms
Roman NettAdvisor: Prof. Bulent YenerTitle: Graph Neural Network Using Local Cell Graph Features for Cancer ClassificationAndy BernhardtAdvisor: Prof. Tomek StrzalkowskiTitle: Imageability as an Indicator of AuthorshipJacy SharlowAdvisor: Prof. Barb CutlerTitle: Automating the Artistic Pipeline Regarding Skin Wrinkling in the Geometric SpaceSteven Laverty Advisor: Prof. Mohammed ZakiTitle: Protein Folding with Deep RLZachary FernandesAdvisor: Prof. Mei SiTitle: Investigating the Impact of Self-Attention on Reinforcement LearningSeth LaurenceauAdvisor: Prof. Ana MilanovaTitle: Verification of Python DocstringsRyan KaplanAdvisor: Prof. Alex GittensTitle: Transfer Learning on Images for Graph Problems
Mohammed Shahid ModiAdvisor: Prof. Bolek SzymanskiTitle: Poster on Dynamics of Ideological Bias Shifts of Users on Social Media PlatformsDhruva Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Understanding forms using graph neural networksHarshaa Hiremagalur NarayanAdvisor: Prof. Mohammed ZakiTitle: Using BERT-GCN on embeddings created using dictionary word
Computer Science Graduate StudentsLally 104   4:00 pm    
 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 
  
    

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


 

Apr
3
2023




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


Apr
3
2023
Apr32023


Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.



Tianfan Fu, Georgia Institute of Technology 
SAGE 5101   12:00 pm


Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.




Deep Learning for Drug Discovery and Development
Artificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.

Deep Learning for Drug Discovery and DevelopmentArtificial intelligence (AI) has become woven into therapeutic discovery to accelerate drug discovery and development processes since the emergence of deep learning. For drug discovery, the goal is to identify drug molecules with desirable pharmaceutical properties. I will discuss our deep generative models that relax the discrete molecule space into a differentiable one and reformulate the combinatorial optimization problem into a differentiable optimization problem, which can be solved efficiently. On the other hand, drug development focuses on conducting clinical trials to evaluate the safety and effectiveness of the drug on human bodies. To predict clinical trial outcomes, I design deep representation learning methods to capture the interaction between multi-modal clinical trial features (e.g., drug molecules, patient information, disease information), which achieves 0.847 F1 score in predicting phase III approval. Finally, I will present my future works in geometric deep learning for drug discovery and predictive model for drug development.Bio: Tianfan Fu is a Ph.D. candidate in the School of Computational Science and Engineering at the Georgia Institute of Technology, advised by Prof. Jimeng Sun. His research interest lies in machine learning for drug discovery and development. Particularly, he is interested in generative models on both small-molecule & macro-molecule drug design and deep representation learning on drug development. The results of his research have been published in leading AI conferences, including AAAI, AISTATS, ICLR, IJCAI, KDD, NeurIPS, UAI, and top domain journals such as Nature, Cell Patterns, Nature Chemical Biology, and Bioinformatics. His work on clinical trial outcome prediction has been selected as the cover paper on Cell Patterns. In addition, Tianfan is an active community builder. He co-organized the first three AI4Science workshop on leading AI conferences (https://ai4sciencecommunity.github.io/); he co-founded Therapeutic Data Commons (TDC) initiative (https://tdcommons.ai/), an ecosystem with AI-solvable tasks, AI-ready datasets, and benchmarks in therapeutic science. Additional information is available at https://futianfan.github.io/.
Tianfan Fu, Georgia Institute of Technology SAGE 5101   12:00 pm    
 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 
  
    

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


 

Mar
29
2023




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


Mar
29
2023
Mar292023


Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.



Siyu Huang, Harvard University 
Sage 3510   11:00 am


Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.




Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data Creation
The rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.

Advancing Data-Aspect AI: From Efficient Data Annotation to High-Quality Data CreationThe rapid progress of deep learning in recent years has led to significant advances in various fields such as computer vision, natural language processing, and speech recognition. The success of deep learning models heavily relies on the availability of large-scale and high-quality datasets. To address this challenge, active learning is a representative strategy that interactively queries human annotators for efficient data annotation. I will discuss how to design both theoretically and empirically effective active learning strategy for deep neural networks. On the other hand, powerful deep learning models have the potential to create high-quality data for human needs nowadays. I will demonstrate this by exploring recent advancements in generative methods. By taking the neural style transfer problem as an example, I will discuss how to achieve a desirable balance between content, style, and visual quality when creating visual contents. I will also share potential future directions of data-aspect AI, as well as the applications to biomedical domains.
Bio: Dr. Siyu Huang is a postdoctoral fellow at the John A. Paulson School of Engineering and Applied Sciences, Harvard University. He received his B.E. and Ph.D. degrees from Zhejiang University in 2014 and 2019, respectively. Prior to joining Harvard, he was a visiting scholar at Carnegie Mellon University in 2018, a research scientist at Baidu Research from 2019 to 2021, and a research fellow at Nanyang Technological University in 2021. His research interests include computer vision, deep learning, and generative AI, with 30 publications on top-tier conferences and journals.
Siyu Huang, Harvard University Sage 3510   11:00 am    
 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 
  
    

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


 

Mar
27
2023




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


Mar
27
2023
Mar272023


Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.



Juan Zhai, Rutgers University 
Sage 5101   12:00 pm


Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.




Software Quality Assessment via Specification Synthesis
Program specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.

Software Quality Assessment via Specification SynthesisProgram specifications provide clear and precise descriptions of behaviors of a software system, which serves as a blueprint for its design and implementation. They help ensure that the system is built correctly and the functions work as intended, making it easier to troubleshoot, modify, and verify the system if needed. NIST suggests that the lack of high-quality specifications is the most common cause of software project failure. Nowadays, successful projects have an equal or even higher number of specifications than code (counted by lines).
 
In this talk, I will present my research on synthesizing both informal and formal specifications for software systems. I will explain how we use a combination of program and natural language semantics to automatically generate informal specifications, even for native methods without implementation in Java which previous methods could not handle. By leveraging the generated specifications, we successfully detect many code bugs and code-comment inconsistencies. Additionally, I will describe how we derive formal specifications from natural language comments using a search-based technique. The generated formal specifications have been applied to facilitate program analysis for existing tools. They have been shown to greatly improve the capabilities of these tools, by detecting many new information leaking paths and reducing false alarms in testing. Overall, the talk will highlight the importance of program specifications in software engineering and demonstrate the potential of our techniques to improve the development and maintenance of software systems.
 
Bio:
Juan Zhai is an Assistant Teaching Professor in the Department of Computer Science at Rutgers University. Previously, she was a Postdoctoral Research Associate, working with Prof. Xiangyu Zhang in the Department of Computer Science at Purdue University. She also worked as a tenure-track Assistant Professor at Nanjing University, where she obtained her Ph.D. degree. Her research interests lie in software engineering, natural language processing, and security, focusing on specification synthesis and enforcement. She is the recipient of the Distinguished Paper Award of USENIX Security 2017 and the Outstanding Doctoral Student Award in NASAC 2016.
Juan Zhai, Rutgers University Sage 5101   12:00 pm    
 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 
  
    

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


 

Mar
20
2023




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


Mar
20
2023
Mar202023


Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.



Maithilee Kunda , Vanderbilt University 
Sage 5101   12:00 pm


Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.




Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinking
While decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.

Reasoning with visual imagery:  Research at the intersection of autism, AI, and visual thinkingWhile decades of AI research on high-level reasoning have yielded many techniques for many tasks, we are still quite far from having artificial agents that can just “sit down" and perform tasks like intelligence tests without highly specialized algorithms or training regimes.  We also know relatively little about how and why different people approach reasoning tasks in different (often equally
successful) ways, including in neurodivergent conditions such as autism. In this talk, I will discuss: 1) my lab's work on AI approaches for reasoning with visual imagery to solve intelligence tests, and what these findings suggest about visual cognition in autism; 2) how imagery-based agents might learn their domain knowledge and problem-solving strategies via search and experience, instead of these components being manually designed, including recent leaderboard results on the very difficult Abstraction & Reasoning Corpus (ARC) ARCathon challenge; and 3) how this research can help us understand cognitive strategy differences in people, with applications related to neurodiversity and employment.  I will also discuss 4) our Film Detective game that aims to visually support adolescents on the autism spectrum in improving their theory-of-mind and social reasoning skills.
 
Bio:  Maithilee Kunda is an assistant professor of computer science at Vanderbilt University. Her work in AI, in the area of cognitive systems, looks at how visual thinking contributes to learning and intelligent behavior, with a focus on applications related to autism and neurodiversity. She directs Vanderbilt’s Laboratory for Artificial Intelligence and Visual Analogical Systems and is a founding investigator in Vanderbilt’s Frist Center for Autism & Innovation.
She has led grants from the US National Science Foundation and the US Institute of Education Sciences and has also collaborated on large NSF Convergence Accelerator and AI Institute projects.  She has published in Proceedings of the National Academy of Sciences (PNAS) and in the Journal of Autism and Developmental Disorders (JADD), the premier journal for autism research, as well as in AI and cognitive science conferences such as ACS, CogSci, AAAI, ICDL-EPIROB, and DIAGRAMS, including a best paper award at the ACS conference in 2020.  Also in 2020, her research on innovative methods for cognitive assessment was featured on the national news program CBS 60 Minutes, as part of a segment on neurodiversity and employment.  She holds a B.S. in mathematics with computer science from MIT and Ph.D. in computer science from Georgia Tech.
Maithilee Kunda , Vanderbilt University Sage 5101   12:00 pm    
 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 
  
    

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


 

Mar
15
2023




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


Mar
15
2023
Mar152023


Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.



Wei Jin, Michigan State University 
SAGE 3510   11:00 am


Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.




Empowering Graph Neural Networks from a Data-Centric View
Many learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.

Empowering Graph Neural Networks from a Data-Centric ViewMany learning tasks in Artificial Intelligence require dealing with graph data, ranging from biology and chemistry to finance and education. As powerful learning tools for graph inputs, graph neural networks (GNNs) have demonstrated remarkable performance in various applications. Despite their success, unlocking the full potential of GNNs requires tackling the limitations of robustness and scalability. In this talk, I will present a fresh perspective on enhancing GNNs by optimizing the graph data, rather than designing new models. Specifically, first, I will present a model-agnostic framework which improves prediction performance by enhancing the quality of an imperfect input graph. Then I will show how to significantly reduce the size of a graph dataset while preserving sufficient information for GNN training.
Wei Jin, Michigan State University SAGE 3510   11:00 am    
 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 
  
    

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


 

Mar
13
2023




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


Mar
13
2023
Mar132023


Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.



Ziniu Hu, University of California, Los Angeles 
Sage 5101   12:00 pm


Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.




Make Knowledge Computable: Towards Differentiable Neural-Symbolic Reasoning
My ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.

Make Knowledge Computable: Towards Differentiable Neural-Symbolic ReasoningMy ultimate research vision is to develop an AI model that can emulate human reasoning and thinking, which requires building a differentiable Neural-Symbolic AI. This approach involves enabling neural models to interact with external symbolic modules, such as knowledge graphs, logical engines, math calculators, and physical/chemical simulators. This will facilitate end-to-end training of such a Neural-Symbolic AI system without annotated intermediate programs.
During this talk, I will introduce my two research endeavors focused on building differentiable neural symbolic AI using knowledge graphs. Firstly, I will discuss how Symbolic Reasoning can help Neural Language Models. I designed OREO-LM, which incorporates knowledge graph relational reasoning into a Large Language Model, significantly improving multi-hop question answering using a single model. Secondly, I will discuss how Neural Embedding can help Symbolic Logic Reasoning. I solve complex first-order logic queries in neural embedding space, using fuzzy logic operators to create a learning-free model that fulfills all logic axioms. Finally, I will discuss my future research plans on applying differentiable neural-symbolic AI to improve program synthesis, architecture design, and scientific discovery. 
Bio:  Ziniu Hu is a fifth-year PhD student in computer science at UCLA. His research focuses on integrating symbolic knowledge reasoning with neural models. Under the guidance of Professors Yizhou Sun and Kai-Wei Chang, he has developed several models that have successfully solved complex question-answering and graph mining problems. His research has received support from Baidu Ph.D. Fellowship and Amazon Ph.D. Fellowship. He also contributed to the research community as the research-track workflow co-chair for KDD'23 and was awarded the top reviewer at NeurIPS'22. His research has been deployed on various industrial applications, including Tiktok unbiased Recommendation, Google YouTube Shorts recommendation, Microsoft Graph anomaly detection, and Facebook hate speech detection service. His research has received several awards, including the best paper award at WWW'19, the best student paper award at DLG-KDD'20 workshop, and the best paper award at SoCal-NLP'22.
Ziniu Hu, University of California, Los Angeles Sage 5101   12:00 pm    
 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 
  
    

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


 

Mar
1
2023




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


Mar
1
2023
Mar12023


Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github



Manling Li , University of Illinois Urbana-Champaign 
SAGE 3510   11:00 am


Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github




Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge Acquisition
Traditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github

Towards Deep Semantic Understanding: Event-Centric Multimodal Knowledge AcquisitionTraditionally, multimodal information consumption has been entity-centric with a focus on concrete concepts (such as objects, object types, physical relations, e.g., a person in a car), but lacks ability to understand abstract semantics (such as events and semantic roles of objects, e.g., driver, passenger, mechanic). However, such event-centric semantics are the core knowledge communicated, regardless whether in the form of text, images, videos, or other data modalities.
At the core of my research in Multimodal Information Extraction (IE) is to bring such deep semantic understanding ability to the multimodal world. My work opens up a new research direction Event-Centric Multimodal Knowledge Acquisition to transform traditional entity-centric single-modal knowledge into event-centric multi-modal knowledge. Such a transformation poses two significant challenges: (1) understanding multimodal semantic structures that are abstract (such as events and semantic roles of objects): I will present my solution of zero-shot cross-modal transfer (CLIP-Event), which is the first to model event semantic structures for vision-language pretraining, and supports zero-shot multimodal event extraction for the first time; (2) understanding long-horizon temporal dynamics: I will introduce Event Graph Model, which empowers machines to capture complex timelines, intertwined relations and multiple alternative outcomes. I will also show its positive results on long-standing open problems, such as timeline generation, meeting summarization, and question answering. Such Event-Centric Multimodal Knowledge Acquisition starts the next generation of information access, which allows us to effectively access historical scenarios and reason about the future. I will lay out how I plan to grow a deep semantic understanding of language world and vision world, moving from concrete to abstract, from static to dynamic, and ultimately from perception to cognition.
Bio: Manling Li is a Ph.D. candidate at the Computer Science Department of University of Illinois Urbana-Champaign. Her work on multimodal knowledge extraction won the ACL'20 Best Demo Paper Award, and the work on scientific information extraction from COVID literature won NAACL'21 Best Demo Paper Award. She was a recipient of Microsoft Research PhD Fellowship in 2021. She was selected as a DARPA Riser in 2022, and a EE CS Rising Star in 2022. She was awarded C.L. Dave and Jane W.S. Liu Award, and has been selected as a Mavis Future Faculty Fellow. She led 19 students to develop the UIUC information extraction system and ranked 1st in DARPA AIDA evaluation in 2019 and 2020. She has more than 30 publications on multimodal knowledge extraction and reasoning, and gave tutorials about event-centric multimodal knowledge at ACL'21, AAAI'21, NAACL'22, AAAI'23, etc. Additional information is available at https://limanling.github
Manling Li , University of Illinois Urbana-Champaign SAGE 3510   11:00 am    
 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
    

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 

Feb
27
2023




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


Feb
27
2023
Feb272023


Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.



Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.




Decentralized intelligence
Today connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.

Decentralized intelligenceToday connected devices, as well as various smart sectors generate a significant amount of data. Tailoring machine learning algorithms to exploit this massive amount of data can lead to many new applications and enable ambient intelligence. The question is how to use this decentralized data to enhance the system intelligence beneficial for everyone while protecting the sensitive information. It is not desirable to offload such massive amounts of data available at the edge devices to a cloud server for centralized processing due to storage, latency, bandwidth, and power constraints, as well as privacy concerns of users. Furthermore, due to the growing storage and computational capabilities of the edge devices, it is increasingly attractive to store and process the data locally by shifting network computations to the edge. This enables decentralized intelligence where local computations on the data converts decentralized data to a global intelligence; hence, enhancing data privacy while learning from the collection of data available across the network. In this talk, I highlight some of the challenges and advances in enabling decentralized intelligence by integrating computations, collaboration, and communications, the three essential components of enabling collective intelligence.  
Bio: Mohammad received the B.Sc. degree in Electrical Engineering from the Iran University of Science and Technology in 2011 and the M.Sc. degree in Electrical and Computer Engineering from the University of Tehran in 2014, both with the highest rank in classes. He also obtained the Ph.D. degree in Electrical and Electronic Engineering at Imperial College London in 2019. He then spent two years as a Postdoctoral Research Associate in the Department of Electrical and Computer Engineering at Princeton University. He is currently a Postdoctoral Associate at MIT where he joined in early 2022. He received the Best Ph.D. Thesis Award from the Department of Electrical and Electronic Engineering at Imperial College London, as well as the IEEE Information Theory Chapter of UK and Ireland in the year 2019. He is also the recipient of the IEEE Communications Society Young Author Best Paper Award (2022) for the paper titled "Federated learning over wireless fading channels". His research interests include machine learning, information theory, distributed computing, privacy and security, and data science.
Mohammad Mohammadi Amiri, Massachusetts Institute of Technology  Sage 5101   12:00 pm    
 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 
  
    

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


 

Feb
23
2023




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


Feb
23
2023
Feb232023


Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 



Alireza Fallah, Massachusetts Institute of Technology  
Sage 5101   12:00 pm


Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 




Data Markets and Learning: Privacy Mechanisms and Personalization
The fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 

Data Markets and Learning: Privacy Mechanisms and PersonalizationThe fuel of machine learning models and algorithms is the data usually collected from users, enabling refined search results, personalized product recommendations, informative ratings, and timely traffic data. However, increasing reliance on user data raises serious challenges. A common concern with many of these data-intensive applications centers on privacy — as a user’s data is harnessed, more and more information about her behavior and preferences is uncovered and potentially utilized by platforms and advertisers. These privacy costs necessitate adjusting the design of data markets to include privacy-preserving mechanisms. 
This talk establishes a framework for collecting data of privacy-sensitive strategic users for estimating a parameter of interest (by pooling users' data) in exchange for privacy guarantees and possible compensation for each user.  We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share her data in exchange for compensation but at the same time has a private heterogeneous privacy cost which we quantify using differential privacy. We consider two popular data market architectures: central and local. In both settings, we use Le Cam's method to establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Next, we pose the mechanism design problem as the optimal selection of an estimator and payments that elicit truthful reporting of users' privacy sensitivities. We further develop efficient algorithmic mechanisms to solve this problem in both privacy settings.
Finally, we consider the case that users are interested in learning different personalized parameters. In particular, we highlight the connections between this problem and the meta-learning framework, allowing us to train a model that can be adapted to each user's objective function.
 
Bio:  Alireza Fallah is a Ph.D. candidate at the department of Electrical Engineering and Computer Science (EECS) and the Laboratory for Information and Decision Systems (LIDS) at Massachusetts Institute of Technology (MIT). His research interests are machine learning theory, data market and privacy, game theory, optimization, and statistics. He has received a number of awards and fellowships, including the Ernst A. Guillemin Best MIT EECS M.Sc. Thesis Award, Apple Scholars in AI/ML Ph.D. fellowship, MathWorks Engineering Fellowship, and Siebel Scholarship. He has also worked as a research intern at the Apple ML privacy team. Before joining MIT, he earned a dual B.Sc. degree in Electrical Engineering and Mathematics from Sharif University of Technology, Tehran, Iran.
 
Alireza Fallah, Massachusetts Institute of Technology  Sage 5101   12:00 pm    
 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 
  
    

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


 

Feb
15
2023




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


Feb
15
2023
Feb152023


Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.



Kirshanthan Sundararajah , Purdue University 
SAGE 3510   11:00 am


Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.




Abstractions for Taming Irregularity at the Top
Addressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.

Abstractions for Taming Irregularity at the TopAddressing the performance gap between software and hardware is one of the major challenges in computer science and engineering. Software stacks and optimization approaches have long been designed targeting regular programs — programs that operate over regular data structures such as arrays and matrices using loops, partly due to the abundance of regular programs in computer software. But irregular programs — programs that traverse over irregular or pointer-based data structures such as sparse matrices, trees, and graphs using a mix of recursion and loops — also appear in many essential applications such as simulation, data mining, graphics, etc. Loop transformation frameworks are good examples of performance-enhancing scheduling transformations for regular programs. Generally, these frameworks reason about transformations in a composable manner (i.e., reason about a sequence of transformations).
 
In the past, scheduling transformations for irregular programs were ad-hoc, and they were considered on the horizon by loop transformation frameworks. Even the few existing ones were applied in isolation, and the composability of these transformations was not studied extensively. In this talk, I will discuss a composable framework for verifying the correctness of scheduling transformations for irregular programs. We will explore the abstractions used in different parts of our framework, and I will show ways to extend these abstractions to capture a wide variety of scheduling transformations for irregular programs. Finally, I will discuss future directions on incorporating dependence analyses and data layout abstractions into this framework.
 
Bio: Kirshanthan (“Krish”) Sundararajah is a PhD candidate in the Elmore Family School of Electrical and Computer Engineering, advised by Milind Kulkarni. He earned his Bachelor's degree from the University of Moratuwa, Sri Lanka, and his Master’s degree from Purdue University. His research interests lie in the areas of compilers, programming languages, and high-performance computing. He is particularly interested in solving the performance challenges of irregular applications. He has published in top conferences such as ASPLOS, OOPSLA, and PLDI and is a recipient of the Bilsland Dissertation Fellowship.
Kirshanthan Sundararajah , Purdue University SAGE 3510   11:00 am    
 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 
  
    

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


 

Feb
13
2023




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


Feb
13
2023
Feb132023


Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 



Hanjie Chen, University of Virginia 
Sage 5101   12:00 pm


Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 




Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLP
Neural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 

Bridging Humans and Machines: Interpretation Techniques for Trustworthy NLPNeural network models have been pushing computers’ capacity limit on natural language understanding and generation while lacking interpretability. The black-box nature of deep neural networks hinders humans from understanding their predictions and trusting them in real-world applications. In this talk, I will introduce my effort in bridging the trustworthy gap between models and humans by developing interpretation techniques, which cover three main phases of a model life cycle—training, testing, and debugging. I will demonstrate the critical values of integrating interpretability into every state of model development: (1) making model prediction behavior transparent and interpretable during training; (2) explaining and understanding model decision-making on each test example; (3) diagnosing and debugging models (e.g., robustness) based on interpretations. I will discuss future directions on incorporating interpretation techniques with system development and human interaction for long-term trustworthy AI.Bio: Hanjie Chen is a Ph.D. candidate in Computer Science at the University of Virginia. Her research interests lie in Trustworthy AI, Natural Language Processing (NLP), and Interpretable Machine Learning. She is a recipient of the Carlos and Esther Farrar Fellowship and the Best Poster Award at the ACM CAPWIC 2021. Her work has been published at top-tier NLP/AI conferences (e.g., ACL, AAAI, EMNLP, NAACL) and selected by the National Center for Women & Information Technology (NCWIT) Collegiate Award Finalist 2021. Besides, as the primary instructor, she co-designed and taught a cross-listed course, CS 4501/6501 Interpretable Machine Learning, at UVA. Her effort in teaching was recognized by the UVA CS Outstanding Graduate Teaching Award and University-wide Graduate Teaching Awards Nominee (top 5% of graduate instructors). 
Hanjie Chen, University of Virginia Sage 5101   12:00 pm    
 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 
  
    

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


 

Feb
8
2023




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


Feb
8
2023
Feb82023


Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.



Maryam Aliakbarpour , Boston University and Northeastern University 
Sage 3704   11:00 am


Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.




Statistical inference with privacy and computational constraints
The vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.

Statistical inference with privacy and computational constraintsThe vast amount of digital data we create and collect has revolutionized many scientific fields and industrial sectors. Yet, despite our success in harnessing this transformative power of data, computational and societal trends emerging from the current practices of data science necessitate upgrading our toolkit for data analysis. In this talk, we discuss how practical considerations such as privacy and memory limits affect statistical inference tasks. In particular, we focus on two examples: First, we consider hypothesis testing with privacy constraints. More specifically, how one can design an algorithm that tests whether two data features are independent or correlated with a nearly-optimal number of data points while preserving the privacy of the individuals participating in the data set. Second, we study the problem of entropy estimation of a distribution by streaming over i.i.d. samples from it. We determine how bounded memory affects the number of samples we need to solve this problem. 
 
Bio:  Maryam Aliakbarpour is a postdoctoral researcher at Boston University and Northeastern University, where she is hosted by Prof. Adam Smith and Prof. Jonathan Ullman. Before that, she was a postdoctoral research associate at the University of Massachusetts Amherst, hosted by Prof. Andrew McGregor (from Fall 2020-Summer 2021). In Fall 2020, she was a visiting participant in the Probability, Geometry, and Computation in High Dimensions Program at the Simons Institute at Berkeley. Maryam received her Ph.D. in September 2020 from MIT, where she was advised by Prof. Ronitt Rubinfeld. Maryam was selected for the Rising Stars in EECS in 2018 and won the Neekeyfar Award from the Office of Graduate Education, MIT.
Maryam Aliakbarpour , Boston University and Northeastern University Sage 3704   11:00 am    
 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 
  
    

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


 

Feb
6
2023




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


Feb
6
2023
Feb62023


Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.



Chengzhi Mao , Columbia University 
Sage 5101   12:00 pm


Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.




Reliable Machine Learning via Integrating Context
Learning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.

Reliable Machine Learning via Integrating ContextLearning-based software and systems are deeply embedded in our lives. However, despite the excellent performance of machine learning models on benchmarks, state-of-the-art methods like neural networks often fail once they encounter realistic settings. Since neural networks often learn correlations without reasoning with the right signals and knowledge, they fail when facing shifting distributions, unforeseen corruptions, and worst-case scenarios. In this talk, I will show how to build reliable and robust machine learning by tightly integrating context into the models. The context has two aspects: the intrinsic structure of natural data, and the extrinsic structure of domain knowledge. Both are crucial: By capitalizing on the intrinsic structure in natural images, I show that we can create robust computer vision systems, even in the worst case, an analytical result that also enjoys strong empirical gains. Through the integration of external knowledge, such as causal structure, my framework can instruct models to use the right signals for visual recognition, enabling new opportunities for controllable and interpretable models. I will also talk about future work in making machine learning robust, which I hope to transform us into an intelligent society.
 
Bio:  Chengzhi Mao is a final-year Ph.D. student from the Department of Computer Science at Columbia University. He is advised by Prof. Junfeng Yang and Prof. Carl Vondrick. He received his B.S in E.E. from Tsinghua University. His research focuses on reliable and robust machine learning. His work has led to over ten publications and Orals at top conferences, which established a new generalization of robust models beyond feedforward inference. His work also connects causality to the vision domain. He serves as reviewers for several top conferences, including CVPR, ICCV, ECCV, ICLR, NeurIPS, IJCAI, and AAAI.
Chengzhi Mao , Columbia University Sage 5101   12:00 pm    
 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 
  
    

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


 

Dec
15
2022




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


Dec
15
2022
Dec152022


Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.



Shlomo Havlin , Bar-Ilan University, Israel 
Low 4034   11:00 am


Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.




Interdependent Networks: Novel Physical Phase Transitions 
A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.

Interdependent Networks: Novel Physical Phase Transitions A framework for studying the percolation theory of interdependent networks will be presented. In interdependent networks, such as infrastructures, when nodes in one network fail, they cause dependent nodes in other networks to also fail. This may happen recursively and can lead to a cascade of failures and to a sudden abrupt fragmentation of the system of interdependent systems. This is in contrast to a single network where the fragmentation percolation transition due to failures is continuous.  I will present analytical solutions based on percolation theory for the critical thresholds, cascading failures, and the giant functional component of a network of n interdependent networks. I will show that the general theory shows many novel processes and features that are not present in the percolation theory of single networks. I will also show that interdependent networks embedded in space are significantly more vulnerable, and the phase transition is much richer compared to non-embedded networks. In particular, small localized attacks of zero fraction but above a critical size may lead to cascading failures that dynamically propagate and yield an abrupt phase transition. I will finally discuss the consequences of the behavior of percolation of interdependent networks on phase transitions in real physical interdependent systems. I will discuss the recent theory and experiments on interdependent superconducting networks where we identified a novel abrupt transition, although each isolated system shows a continuous transition.
References:
[1] S. Buldyrev, G. Paul, H.E. Stanley, S. Havlin, Nature, 464, 08932 (2010).
[2] Jianxi Gao, S. Buldyrev, H. E. Stanley, S. Havlin, Nature Physics, 8, 40 (2012).
[3] A. Bashan et al, Nature Physics, 9, 667 (2013) [4] A Majdandzic et al, Nature Physics 10 (1), 34 (2014); Nature Comm. 7, 10850 (2016) [5] M. Danziger et al, Nature Physics  15(2), 178 (2019) [6] I Bonamassa et al, Interdependent superconducting networks, preprint arXiv:2207.01669 (2022) [7] B. Gross et al,  arXiv:2208.00440, PRL, in press (2022)
 
Bio:
Professor Shlomo Havlin has made fundamental contributions to the physics of complex systems and statistical physics. These discoveries have impacted many other fields, such as medicine, biology, geophysics, and more. He has over 60,000 citations on ISI Web of Science and over 100,000 in Google Scholar. His h-index is 112 (142) in Web of Science (Google Scholar). Professor Havlin has been a Highly Cited Scientist in the last 3 years.
He is a professor in the Physics Department at Bar-Ilan University. He received his PhD in 1972 from Bar Ilan University, and he has been a professor at BIU since 1984. Also, between the years of 1999 to 2001, he was the Dean of the Faculty of Exact Sciences, and from 1996 to 1999, he was the President of the Israel Physical Society. Professor Havlin is an IOP Honorary Fellow (England, 2021). He won the Senior Scientific Award of the International Complex Systems Society, the Israel prize in Physics (2018), Order of the Star of Italy, President of Italy (2017), the Rothschild Prize for Physical and Chemical Sciences, Israel (2014), the Lilienfeld Prize for "a most outstanding contribution to physics," APS, USA (2010), the Humboldt Senior Award, Germany (2006), the Distinguished Scientist Award, Chinese Academy of Sciences (2017), the Weizmann Prize for Exact Sciences, Israel (2009), the Nicholson Medal, American Physical Society, USA (2006), and many others.
Professor Havlin has been a leading pioneer in the development of network science, with over 800 papers and books in the fields of statistical physics, network science, and interdisciplinary physical sciences. His main research interests in the last 12 years have focused on interdependent networks, cascading failures, networks of networks, and their implications to real-world problems. The real-world systems applications include physiology, climate, infrastructures, finance, traffic, earthquakes, and others.
Shlomo Havlin , Bar-Ilan University, Israel Low 4034   11:00 am    
 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 
  
    

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


 

Dec
7
2022




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


Dec
7
2022
Dec72022


How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.



Neil Heffernan, Worcester Polytechnic Institute 
Low CII 3051   4:00 pm


How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.




How can Platforms like ASSISTments be used to Improve Research
Abstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.

How can Platforms like ASSISTments be used to Improve ResearchAbstract: The head of the Institute of Education Sciences has asked how we can use platform<https://www.the74million.org/article/schneider-garg-medical-researchers-find-cures-by-conducting-many-studies-and-failing-fast-we-need-to-do-the-same-for-education/>s to increase education sciences. We at ASSISTments have been addressing this<https://www.the74million.org/article/heffernan-how-can-we-know-if-ed-tech-works-by-encouraging-companies-researchers-to-share-data-government-funding-can-help/> very question. How do platforms like EdX, Khan Academy, or Canvas improve science? There is a crisis in American science referred to as the Reproducibility Crisis where many experimental results cannot be reproduced. We are trying to address that crisis by helping “good science” be done. People who control platforms have a responsibility to try to make them useful tools for learning what works. In Silicon Valley, every company is doing AB Testing to refine their individual products. That, in and of itself, is a good thing and we should use these platforms to figure out how to make them more effective. One of the ways we should do that is by experimenting with different ways of helping students succeed. ASSISTments<http://www.assistments.org/>, a platform I have created with 450,000 middle-school math students, is used to help scientists run studies. I will explain how we have over 100 experiments running inside the ASSISTments platform and how the ASSISTmentsTestBed.org allows external researchers to propose studies. I will also explain how proper oversight is done by our Institutional Review Board. Further, I will explain how users of this platform agree ahead of time to OpenScience procedures such as open-data, open-materials and pre-registration. I’ll illustrate some examples with the twenty-four randomized controlled trials<https://www.etrialstestbed.org/> that I have published as well as the three studies that have more recently come out from the platform by others. Finally, I will point to how we are anonymizing our data and how over 34 different external researchers have used our datasets to publish scientific studies<https://sites.google.com/site/assistmentsstudies/useourdata>. I would like to thank the U.S. Department of Education and the National Science Foundation for their support of over $32 million from 40+ grants.
Bio: Neil Heffernan is William Smith Dean's Professor of Computer Science and Director of the Learning Sciences & Technology program at Worcester Polytechnic Institute. He co-founded ASSISTments, a web-based learning platform, which he developed not only to help teachers be more effective in the classroom,  but also so that he could use the platform to conduct studies to improve the quality of education. Professor Heffernan is passionate about educational data mining and enjoys supervising WPI students, helping them create ASSISTments content and features. Several student projects have resulted in peer-reviewed publications that compare different ways to optimize student learning. Professor Heffernan's goal is to give ASSISTments to millions across the U.S. and internationally as a free service.
Neil Heffernan, Worcester Polytechnic Institute Low CII 3051   4:00 pm    
 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
    

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


 

Dec
2
2022




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


Dec
2
2022
Dec22022


Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett



Computer Science Graduate Students
Lally 102   4:00 pm


Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett




Computer Science Poster Session
Muhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett

Computer Science Poster SessionMuhammad Saad Atique
Taming Uncertainty in Social Networks
Advisor: Prof. Bolek Szymanski
 
Sahith Bhamidipati
Generating Synthetic Election Data
Advisor: Prof. Lirong Xia
 
Matthew Crotty
Determining Image Hardness using Ensemble Classifier Method
Advisor: Prof. Radoslav Ivanov
 
Olivia Lundelius
An Analysis of Improved Daltonization Algorithms
Advisor: Prof. Barb Cutler
 
Nicholas Lutrzykowski
Driver for Seizure Prediction Using EEG Data
Advisor: Prof. Bulent Yener
 
Richard Pawelkiewicz
Action at a Distance
Advisor: Prof. Bulent Yener
 
Noah Prisament
A Variation on the Hotelling-Downs Model with Facility Synergy: the Mall Effect
Advisor: Prof. Elliot Anshelevich
 
Jeff Putlock
Overcoming Missing Data and Labels During VFL
Advisor: Prof. Stacy Patterson
 
Vijay Sadashivaiah
Towards Explainable Transfer Learning
Advisor: Prof. Jim Hendler
 
Bishwajit Saha
Clustering with Associative Memories
Advisor: Prof. Mohammed Zaki
 
Mara Schwartz
Topical Analysis of Twitter User Clusters
Advisor: Prof. Tomek Strzalkowski
 
Xiao Shou
Event Former: A Self-Supervised Learning Paradigm for Temporal Point Process
Advisor: Prof. Kristin Bennett
Computer Science Graduate StudentsLally 102   4:00 pm    
 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 
  
    

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


 

Nov
29
2022




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


Nov
29
2022
Nov292022


Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 



Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland 
Sage 4101   4:00 pm


Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 




Computer Vision Applications at LUT University
The presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 

Computer Vision Applications at LUT UniversityThe presentation considers computer vision, especially a point of view of applications. Digital image processing and analysis with machine learning methods enable efficient solutions for various areas of useful data-centric engineering applications. Challenges with domain adaptation, active learning,  open set classification, and metric learning of similarities are considered. Computer Vision and Pattern Recognition Laboratory at LUT focuses on industrial computer vision, biomedical engineering, social signal processing, and data analytics. Different applications are given as examples based on specific data: fundus images in diagnosis of diabetic retinopathy, planktons in the Baltic Sea, Saimaa ringed seals in the Lake Saimaa, and logs in the sawmill industry.
Bio: Heikki Kälviäinen has been a Professor of Computer Science and Engineering since 1999. He is the head of the Computer Vision and Pattern Recognition Laboratory (CVPRL) at the Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland. He received his D.Sc. (Tech.) degree in Computer Science and Engineering in 1994 from the Department of Information Technology of LUT. Prof. Kälviäinen's research interests include computer vision, machine vision, pattern recognition, machine learning, and digital image processing and analysis.
 
Heikki Kälviäinen , Department of Computational Engineering of Lappeenranta-Lahti University of Technology LUT, Finland Sage 4101   4:00 pm    
 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 
  
    

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


 

Nov
16
2022




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


Nov
16
2022
Nov162022


Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 



Joshua Booth, University of Alabama Huntsville 
https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm


Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 




Approximate Computing of Sparse Matrix Orderings via Neural Acceleration
Determining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 

Approximate Computing of Sparse Matrix Orderings via Neural AccelerationDetermining an ideal ordering for a sparse matrix is difficult. In the case of finding an ordering to minimize fill-in for factorization of a general sparse matrix, the problem is NP-Hard. Meanwhile, the selection of an ordering for an iterative method, such as Conjugate Gradients, that reduces iteration counts and efficiently uses the cache hierarchy is based on observations and rules of thumb. The selection of an ordering is exacerbated in applications that solve a series of sparse matrix problems that change over time, e.g., those found in some circuit simulations. Modern computing systems tend to be heterogeneous containing accelerators, such as a GPU or neural device, that can be co-scheduled with the main CPU. These accelerators are ideal for the computation of complex artificial neural networks. This work uses these accelerators to construct a neural network model that approximates an ordering for a given sparse matrix. This approximation model acts at accelerating the selection of an ordering during the application. Moreover, a trained model can be used iteratively in the case of applications where sparse matrices evolve during execution to determine if a new ordering should be implemented at some point in the computation to speed up the application. 
 
Speaker Bio:
Joshua Booth is an Assistant Professor of Computer Science at the University of Alabama in Huntsville. He received his Ph.D. in Computer Science and Engineering at The Pennsylvania State University and was a Post-Doctoral Researcher in the Scalable Algorithm Division at Sandia National Laboratories where he constructed new sparse linear solver methods for their exascale computing initiative. Dr. Booth is deeply committed to teaching emerging computing systems and technologies to future generations and spent several years teaching at the top liberal art colleges of Bucknell University and Franklin & Marshall College.  His research focuses on high-performance computing related to scalable sparse linear algebra, scheduling, resiliency, and system performance.His contributions and potential have been recognized via being awarded a bronze-level ACM Graduate Student Research Award for his work on multilevel preconditioning and an NSF CAREER award related to the neural acceleration of irregular applications. Additionally, Dr. Booth is an active member of the community leading the Huntsville Computer Research Seminar and reviewing for numerous ACM and IEEE journals 
Joshua Booth, University of Alabama Huntsville https://rensselaer.webex.com/rensselaer/j.php?MTID=mcb603fc4a633ac4882d14146932bd355   Password: colloquium    4:00 pm    
 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 
  
    

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


 

Nov
9
2022




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


Nov
9
2022
Nov92022


Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.



Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute 
Carnegie 113   4:00 pm


Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.




Information-theoretic computational rationality
The human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.

Information-theoretic computational rationalityThe human mind is a remarkable thing. On the one hand, even "mundane" aspects of cognition, such as our ability to fold clean laundry, far outstrip the capabilities of the most advanced robotic systems. On the other hand, we know from decades of research in psychology that the human mind is also astonishingly limited. Human minds face strict limits on attention, working memory, perception, and other basic cognitive faculties. In this talk, I will discuss a framework for resolving this apparent tension that spans the fields of cognitive science and artificial intelligence, known as computational rationality. According to this framework, the mind utilizes computations and algorithms that maximize the expected utility of behavior, while subject to constraints on the ability to store, manipulate, and process information. In my research, I have focused primarily on the use of rate-distortion theory (a sub-field of information theory) to characterize these limits on human information processing. The approach will be demonstrated through computational models of perception, memory, decision making, and reinforcement learning.Bio: Chris Sims received a B.S. in computer science from Cornell University (2003), followed by a Ph.D. in Cognitive Science from Rensselaer Polytechnic Institute (2009). After completing his Ph.D., Dr. Sims held a postdoctoral research position at the University of Rochester, and a faculty position at Drexel University before joining the faculty at RPI in 2017. Dr. Sims's overarching research interest lies in developing computational models of human intelligence. Specific research areas include reinforcement learning, visual memory and perceptual expertise, sensory-motor control and motor learning, and learning and decision-making under uncertainty.
Christopher Sims, Assistant Professor, Rensselaer Polytechnic Institute Carnegie 113   4:00 pm    
 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Apr
21
2022




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


Apr
21
2022
Apr212022


Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.



Lei Yu, IBM Thomas J. Watson Research 
WebEx - link to be emailed ahead of seminar   10:00 am


Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.




Reconciling Privacy and Utility for Big Data Applications
The proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.

Reconciling Privacy and Utility for Big Data ApplicationsThe proliferation of mobile devices with sensing and tracking capabilities, cloud computing, smart home, and the internet of things (IoT) has been fundamentally digitizing people’s daily life, and unprecedentedly extending the scale of personal data collection. While vast amounts of individual data have been turned into fuels with big data technologies to create and drive business, they have brought forth vital privacy concerns and real risks.  At the same time, it remains a significant challenge to effectively preserve data privacy with regulatory compliance while ensuring data utility effectively. In this talk, I will discuss privacy risks in different phases of a data life cycle, from data collection, and usage to publication. Along this line, I will go through our works on differential location privacy for location-based services, differentially private model publishing for deep learning, and differentially private location trace synthesis for location data publication. Finally, I will discuss my vision of future research opportunities for data privacy in the big data era. 
Biography:  Lei Yu is a Research Staff Member at IBM Thomas J. Watson Research. He received his Ph.D. from the school of computer science at Georgia Insitute of Technology. His research interests include data privacy, the security and privacy of machine learning, and mobile and cloud computing. His works were published in top-tier conferences, including IEEE S&P, CCS, NDSS, and INFOCOM. At IBM, his work focuses on log analytics, system anomaly detection, and data privacy for enterprise systems. He is a recipient of the 2021 IBM Research Accomplishment award.
Lei Yu, IBM Thomas J. Watson Research WebEx - link to be emailed ahead of seminar   10:00 am    
 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 
  
    

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


 

Apr
20
2022




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


Apr
20
2022
Apr202022


Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.



Chong Liu, University of California, Santa Barbara (UCSB) 
https://rensselaer.webex.com/meet/xial   11:00 am


Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.




Doubly Robust Crowdsourcing and Beyond
Crowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.

Doubly Robust Crowdsourcing and BeyondCrowdsourcing studies how to aggregate a large collection of votes, which makes large-scale labeled datasets and modern AI possible. In this talk, I will mainly present our recent JAIR paper which focuses on labeling cost reduction problem of crowdsourcing. First, we reformulate the crowdsourcing problem as a statistical estimation problem where votes are approximated by worker models. Then doubly robust estimation is used to address this problem. We prove that the variance of estimation can be substantially reduced, hence the labeling cost can be reduced, even if the worker model is a poor approximation. Moreover, with adaptive worker/item selection rules, labeling cost can be further reduced. I conclude by summarizing future directions of doubly robust crowdsourcing and introducing more applications of voting, for example, private learning.Bio:Chong Liu is a PhD candidate in Computer Science at University of California, Santa Barbara (UCSB), advised by Prof. Yu-Xiang Wang. His research interests include ensemble learning, active learning, and global optimization. He has served on program committees of several conferences, e.g., AISTATS, AAAI, ICML, and NeurIPS. He is also serving on the editorial board of JMLR.
Chong Liu, University of California, Santa Barbara (UCSB) https://rensselaer.webex.com/meet/xial   11:00 am    
 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 
  
    

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


 

Apr
8
2022




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


Apr
8
2022
Apr82022


Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan



Computer Science Graduate Students
Lally 102   4:00 pm


Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan




Graduate Student Poster Session
Presenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan

Graduate Student Poster SessionPresenters will be:
Aidan Lane
Fuzzy Search on Encrypted DataAdvisor: Konstanin Kuzmin
Leon MontealegreOpenCircuits EEAdvisor: Wes TurnerDaniel StevensArticle Similarity DetectionAdvisor: Sibel Adali
Caitlin CrowleyHuman Assisted FuzzingAdvisor: Bulent YenerOmar MalikResource-mediated Consensus Formation on Random NetworksAdvisor: Bolek SzymanskiMack QianMaterial Point Method for Continuum Material SimulationAdvisor: Barb CutlerBrian HotoppEfficiently Computing and Visualizing Semantic ShiftAdvisor: Sibel AdaliAlex SidgwickMemory Optimization for ECS ArchitectureAdvisor: Jasmine PlumInwon KangDetecting Preference in Text using Dependency and Co-ReferenceAdvisor: Lirong XiaAaron CockleyTBDAdvisor: Bulent YenerSiwen ZhangStudy on lung lesions detection and characterization using open source toolkitAdvisor: Wes Turner 
Tobias ParkServerless Federated LearningAdvisor: Stacy PattersonDaniel JanikowskiSimulation of Light Transport through Opal Type MaterialsAdvisor: Barb CutlerJianan LinPNE and POA in Hotelling’s Game with Weighted Cost FunctionsAdvisor: Elliot AnshelevichStephen TrempelTBDAdvisor: Bulent Yener and Brian Callahan
Computer Science Graduate StudentsLally 102   4:00 pm    
 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Apr
8
2022




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


Apr
8
2022
Apr82022


Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.



Sourav Medya, Northwestern University 
WebEx - link to be emailed ahead of seminar   10:00 am


Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.




Optimization and Learning on Graphs
Networks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.

Optimization and Learning on GraphsNetworks (or graphs) are a powerful tool to model complex systems such as social networks, transportation networks, and the Web. The accurate modeling of such systems enables us to improve infrastructure, reduce conflicts in social media, and make better decisions in high-stakes settings. However, as graphs are highly combinatorial structures, these optimization and learning tasks require the design of efficient algorithms. 
In this talk, I will describe three research directions in the context of network data. First, I will overview several combinatorial problems for graph optimization that I have addressed using classical approaches such as approximate and randomized algorithms. The second part will focus on a different and a more recent approach to solving combinatorial problems by leveraging the power of machine learning. More specifically, I will show how combining neural architectures on graphs with reinforcement learning solves popular data ming problems such as the influence maximization problem.  In the last one, I will demonstrate how to deploy these methods on problems in computational social science with applications in decision-making for patent review systems and the stock market. 
 
Bio: Sourav Medya is a research assistant professor in the Kellogg School of Management at Northwestern University. He is also affiliated with the Northwestern Institute of Complex Systems. He has received his Ph.D. in Computer Science at the University of California, Santa Barbara. His research has been published at several venues including VLDB, NeurIPS, WebConf (WWW), AAMAS, IJCAI, WSDM, SDM, ICDM, SIGKDD Explorations, and TKDE. He has also been a PC member for WSDM, WebConf, AAAI, SDM, AAMAS, ICLR, and IJCAI. 
Sourav's research is focused on the problems at the intersection of graphs and machine learning. More specifically he designs data science tools that optimize graph-based processes and improve the quality as well as scalability of traditional graph combinatorial and mining problems. He also deploys these tools to solve problems in the interdisciplinary area of computational social science especially to improve innovation.
Sourav Medya, Northwestern University WebEx - link to be emailed ahead of seminar   10:00 am    
 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Mar
22
2022




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


Mar
22
2022
Mar222022


Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 



Ben Greenman, Brown University 
WebEx - link to be emailed ahead of seminar   10:00 am


Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 




Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and People
For over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 

Mixing Typed and Untyped Code: A Tale of Proofs, Performance, and PeopleFor over 50 years now, the choice between typed and untyped code has been the source of lively debates. Languages have therefore begun to support both styles, but doing so presents new challenges at the boundaries that link typed and untyped pieces. The challenges stem from three conflicting dimensions: the expressiveness of typed-untyped mixes, the guarantees that types provide, and the cost of enforcing the guarantees. Even though dozens of languages explore points in this complex design space, they tend to focus on one dimension and neglect the others, leading to a disorganized research landscape.
In this talk, I introduce principled methods to guide the design of languages that mix typed and untyped code. The methods characterize both the behaviors of static types and the run-time cost of enforcing them, and do so in a way that centers on their implications for developers. I have applied these methods to improve existing languages and to implement a new language that bridges major gaps in the design space. My ongoing work is using insights from programmers to drive further advances.
BIO:  Ben Greenman is a postdoc at Brown University. He received his Ph.D. from Northeastern University in 2020, and B.S. and M.Eng. degrees from Cornell. His work is supported by the CRA CIFellows program and has led to collaborations with Instagram and RelationalAI.
 
Ben Greenman, Brown University WebEx - link to be emailed ahead of seminar   10:00 am    
 

Mar
15
2022







 
  
    

Mar
15
2022







 

Mar
15
2022







Mar
15
2022
Mar152022




    
 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Mar
15
2022




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


Mar
15
2022
Mar152022


Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 



Shu Kong, Carnegie-Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 




Open-World Visual Perception
Visual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 

Open-World Visual PerceptionVisual perception is indispensable in numerous applications such as autonomous vehicles. Today's visual perception algorithms are often developed under a closed-world paradigm (e.g., training machine-learned models over curated datasets), which assumes the data distribution and categorical labels are fixed a priori. This assumption is unrealistic in the real open world, which contains situations that are dynamic and unpredictable. As a result, closed-world visual perception systems appear to be brittle in the open-world. For example, autonomous vehicles with such systems could fail to recognize a never-before-seen overturned truck and crash into it. We are motivated to ask how to (1) detect all the object instances in the image, and (2) recognize the unknowns. In this talk, I will present my solutions and their applications in autonomous driving and natural science. I will also introduce more research topics in the direction of Open-World Visual Perception.BioShu Kong is a Postdoctoral Fellow in the Robotics Institute at Carnegie-Mellon University, supervised by Prof. Deva Ramanan. He earned a Ph.D. in Computer Science at the University of California-Irvine, advised by Prof. Charless Fowlkes. His research interests span computer vision and machine learning, and their applications to autonomous vehicles and natural science. His current research focuses on Open-World Visual Perception. His recent paper on this topic received Best Paper / Marr Prize Honorable Mention at ICCV 2021. He regularly serves on the program committee in major conferences of computer vision and machine learning. He also serves as the lead organizer of workshops on Open-World Visual Perception at CVPR 2021 and 2022. His latest interdisciplinary research includes building a high-throughput pollen analysis system, which was featured by the National Science Foundation as that "opens a new era of fossil pollen research".
 
Shu Kong, Carnegie-Mellon University WebEx - link to be emailed ahead of seminar   10:00 am    
 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
    

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


 

Feb
28
2022




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


Feb
28
2022
Feb282022


Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.



Xinya Du, University of Illinois at Urbana-Champaign 
WebEx - link to be emailed ahead of seminar   11:00 am


Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.




Towards More Intelligent Extraction of Information from Documents
Large amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.

Towards More Intelligent Extraction of Information from DocumentsLarge amounts of text are written and published daily. As a result, applications such as reading through the documents to automatically extract useful and structured information from the text have become increasingly needed for people’s efficient absorption of information. They are essential for applications such as answering user questions, information retrieval, and knowledge base population.
In this talk, I will focus on the challenges of finding and organizing information about events and introduce my research on leveraging knowledge and reasoning for document-level information extraction. In the first part, I’ll introduce methods for better modeling the knowledge from context: (1) generative learning of output structures that better model the dependency between extracted events to enable more coherent extraction of information (i.e., event A happening in the earlier part of the document is usually correlated with event B in the later part). (2) How to utilize information retrieval to enable memory-based learning with even longer context.
In the second part, to better access relevant external knowledge encoded in large models for reducing the cost of human annotations, we propose a new question-answering formulation for the extraction problem. I will conclude by outlining a research agenda for building the next generation of efficient and intelligent machine reading systems with close to human-level reasoning capabilities.
 
 
Bio:
Xinya Du is a Postdoctoral Research Associate at the University of Illinois at Urbana-Champaign working with Prof. Heng Ji. He earned a Ph.D. degree in Computer Science from Cornell University, advised by Prof. Claire Cardie. Before Cornell, he received a bachelor's degree in Computer Science from Shanghai Jiao Tong University. His research is on natural language processing, especially methods that leverage knowledge & reasoning skills for document-level information extraction. His work has been published in leading NLP conferences such as ACL, EMNLP, NAACL and has been covered by major media like New Scientist. He has received awards including the CDAC Spotlight Rising Star award and SJTU National Scholarship.
Xinya Du, University of Illinois at Urbana-Champaign WebEx - link to be emailed ahead of seminar   11:00 am    
 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


 

Feb
24
2022




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


Feb
24
2022
Feb242022


Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.



Elizabeth Bondi , Harvard University  
WebEx - link to be emailed ahead of seminar   10:00 am


Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.




Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and Conservation
AI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.

Translating AI to Impact: Uncertainty and Human-agent Interactions in Multi-agent Systems for Public Health and ConservationAI is now being applied widely in society, including to support decision-making in important, resource-constrained efforts in conservation and public health. Such real-world use cases introduce new challenges, like noisy, limited data and human-in-the-loop decision-making. I show that ignoring these challenges can lead to suboptimal results in AI for social impact systems. For example, previous research has modeled illegal wildlife poaching using a defender-adversary security game with signaling to better allocate scarce conservation resources. However, this work has not considered detection uncertainty arising from noisy, limited data. In contrast, my work addresses uncertainty beginning in the data analysis stage, through to the higher-level reasoning stage of defender-adversary security games with signaling. I introduce novel techniques, such as additional randomized signaling in the security game, to handle uncertainty appropriately, thereby reducing losses to the defender. I show similar reasoning is important in public health, where we would like to predict disease prevalence with few ground truth samples in order to better inform policy, such as optimizing resource allocation. In addition to modeling such real-world efforts holistically, we must also work with all stakeholders in this research, including by making our field more inclusive through efforts like my nonprofit, Try AI.
Bio: Elizabeth Bondi is a PhD candidate in Computer Science at Harvard University advised by Prof. Milind Tambe. Her research interests include multi-agent systems, remote sensing, computer vision, and deep learning, especially applied to conservation and public health. Among her awards are Best Paper Runner up at AAAI 2021, Best Application Demo Award at AAMAS 2019, Best Paper Award at SPIE DCS 2016, and an Honorable Mention for the NSF Graduate Research Fellowship Program in 2017.
Elizabeth Bondi , Harvard University  WebEx - link to be emailed ahead of seminar   10:00 am    
 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 
  
    

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


 

Feb
22
2022




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


Feb
22
2022
Feb222022


Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.



Nengkun Yu,  University of Technology Sydney 
WebEx - link to be emailed ahead of seminar   4:00 pm


Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.




Efficient verification and testing of quantum systems
Quantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.

Efficient verification and testing of quantum systemsQuantum can solve complex problems that classical computers will never be able to. 
In recent years, significant efforts have been devoted to building quantum computers to solve 
real-world problems. To ensure the correctness of quantum systems, we develop the verification 
techniques and testing algorithms for quantum systems. In the first part of this talk, I will overview 
my work of efficient reasoning about quantum programs by developing verification techniques and 
tools that leverage the power of Birkhoff & von Neumann quantum logic. In the second part, I will 
review my work on quantum state tomography, i.e., learning the classical description of quantum 
hardware, which closes a 40 years long-standing gap between the upper and lower boundsfor quantum state engineering.
 
Bio: Nengkun Yu is an associate professor in the Centre for Quantum Software and Information,  
the University of Technology Sydney. He received his B.S. and PhD degrees from the Department 
of Computer Science and Technology, Tsinghua University, Beijing, China, in July of 2008 and 2013. 
He won the ACM SIGPLAN distinguished paper award at OOPSLA 2020 and the ACM SIGPLAN 
distinguished paper award at PLDI 2021. His research interest focuses on quantum computing.
Nengkun Yu,  University of Technology Sydney WebEx - link to be emailed ahead of seminar   4:00 pm    
 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Feb
17
2022




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


Feb
17
2022
Feb172022


Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.



Xiaorui Liu , Michigan State University 
WebEx - link to be emailed ahead of seminar   10:00 am


Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.




Efficient and Secure Message Passing for Machine Learning


Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.

Efficient and Secure Message Passing for Machine Learning

Message passing is the essential building block in many machine learning problems such as decentralized learning and graph neural networks. In this talk, I will introduce several innovative designs of message passing schemes that address the efficiency and security issues in machine learning. Specifically, first I will present a novel decentralized algorithm with compressed message passing that enables large-scale, efficient, and scalable distributed machine learning on big data. Then I will show how to significantly improve the security and robustness of graph neural networks by exploiting the structural information in data with a novel message passing design.
 
Biography: Xiaorui Liu is a Ph.D. candidate in the Department of Computer Science and Engineering at Michigan State University. His advisor is Prof. Jiliang Tang. His research interests include distributed and trustworthy machine learning, with a focus on big data and graph data. He was awarded the Best Paper Honorable Mention Award at ICHI 2019, MSU Engineering Distinguished Fellowship, and Cloud Computing Fellowship. He organized and co-presented four tutorials in KDD 2021, IJCAI 2021, and ICAPS 2021, and he has published innovative works in top-tier conferences such as NeurIPS, ICML, ICLR, KDD, and AISTATS. More information can be found on his homepage https://cse.msu.edu/~xiaorui/.
Xiaorui Liu , Michigan State University WebEx - link to be emailed ahead of seminar   10:00 am    
 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Feb
10
2022




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


Feb
10
2022
Feb102022


Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.



Pratik Soni, Carnegie Mellon University 
WebEx - link to be emailed ahead of seminar   10:00 am


Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.




Foundations of Advanced Cryptography
Today, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.

Foundations of Advanced CryptographyToday, cryptography has transcended beyond protecting data in transit. Many advanced cryptographic techniques are gaining traction due to their significance in emerging systems like blockchains, privacy-preserving computation, and cloud computing. However, designing cryptography for these systems is challenging as they require underlying cryptographic algorithms to provide strong security and privacy guarantees and admit very efficient implementations.In the first part of this talk, I will present my work that explores fundamental connections between cryptography and blockchains. Blockchains rely on advanced cryptography like Zero-knowledge Proofs, and despite amazing advances in building efficient cryptographic tools, scalability is a major challenge plaguing blockchain-based applications like cryptocurrencies. I will discuss my work on improving prover’s time and memory overheads in Zero-knowledge Proofs, which is currently a primary bottleneck towards building more efficient zero-knowledge proofs. Then, I will discuss my work where I use blockchains to build new and useful cryptography.Finally, I will discuss my work on designing cryptographic commitments, digital analogs of sealed envelopes, secure against man-in-the-middle attacks. While heuristic constructions are known, my work introduces new fundamental techniques to circumvent strong barriers established for achieving provably secure protocols with minimal interaction. Specifically, I build provably secure protocols that require minimal (to no) interaction between the sender and the receiver.Bio:Pratik Soni is a Postdoctoral Research Fellow in the School of Computer Science at Carnegie Mellon University. He received his Ph.D. from UC Santa Barbara in 2015. His research interests span a wide range of topics across cryptography, including zero-knowledge proofs, non-malleable cryptography, secure multi-party computation, and its connections with blockchain technology. His work at FOCS 2017 was invited to SIAM Journal of Computing's special issue, and he is currently serving as a Program Committee Member at ACM CCS 2022 and ASIACRYPT 2022.
Pratik Soni, Carnegie Mellon University WebEx - link to be emailed ahead of seminar   10:00 am    
 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Feb
8
2022




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


Feb
8
2022
Feb82022


Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.



Wenbo Guo, Penn State 
WebEx - link to be emailed ahead of seminar   10:00 am


Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.




Strengthening and Enriching Machine Learning for Cybersecurity
Nowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.

Strengthening and Enriching Machine Learning for CybersecurityNowadays, security researchers are increasingly using AI to automate and facilitate security analysis. Although making some meaningful progress, AI has not maximized its capability in security yet due to two challenges. First, existing ML techniques have not reached security professionals' requirements in critical properties, such as interpretability and adversary-resistancy. Second, Security data imposes many new technical challenges, which break the assumptions of existing ML Models and thus jeopardize their efficacy. 
In this talk, I will describe my research efforts to address the above challenges, with a primary focus on strengthening the interpretability of deep neural networks and deep reinforcement learning policies. Regarding deep neural networks, I will describe an explanation method for deep learning-based security applications and demonstrate how security analysts could benefit from this method to establish trust in blackbox models and conduct efficient finetuning. As for DRL policies, I will introduce a novel approach to draw critical states/actions of a DRL agent and show how to utilize the above explanations to scrutinize policy weaknesses, remediate policy errors, and even defend against adversarial attacks. Finally, I will conclude by highlighting my future plan towards strengthening the critical properties of advanced ML techniques and maximizing their capability in cyber defenses.
 
Bio:
Wenbo Guo is a Ph.D. Candidate at Penn State, advised by Professor Xinyu Xing. His research interests are machine learning and cybersecurity. His work includes strengthening the fundamental properties of machine learning models and designing customized machine learning models to handle security-unique challenges. He is a recipient of the IBM Ph.D. Fellowship (2020-2022), Facebook/Baidu Ph.D. Fellowship Finalist (2020), and ACM CCS Outstanding Paper Award (2018). His research has been featured by multiple mainstream media and has appeared in a diverse set of top-tier venues in security, machine learning, and data mining. Going beyond academic research, he also actively participates in many world-class cybersecurity competitions and has won the 2018  DEFCON/GeekPwn AI challenge finalist award.
Wenbo Guo, Penn State WebEx - link to be emailed ahead of seminar   10:00 am    
 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Feb
4
2022




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


Feb
4
2022
Feb42022


“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 



Yu Kong, Rochester Institute of Technology 
WebEx - link to be emailed ahead of seminar   10:00 am


“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 




“Open Your Eyes”: Towards Visual Understanding in Open World
Our visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 

“Open Your Eyes”: Towards Visual Understanding in Open WorldOur visual world is naturally open, containing visual elements that are dynamic, vast, and unpredictable. However, existing computer vision models are often developed inside a closed-world paradigm, for example, recognizing objects or human actions from a fixed set of categories. If these models are exposed to the realistic complexity of the open world, they will be brittle and fail to generalize. For example, an autonomous driving vehicle may not be able to avoid an accident if it sees a turnover truck, because it has never seen this novelty in its training data. In order to enable visual understanding in open world, vision models need to be aware of the unknown novelties, interpret their decision processes to human users, and adapt themselves to the novelties. In this talk, I will describe our recent work on open-set recognition and interpretable visual precognition. Together, these approaches make strides towards assurance autonomous AI in open world.
Bio: Dr. Yu Kong is now an Assistant Professor directing the ACTION lab in the Golisano College of Computing and Information Sciences at Rochester Institute of Technology. He received B.Eng. degree in automation from Anhui University in 2006, and PhD degree in computer science from Beijing Institute of Technology, China, in 2012. He was a postdoctoral research associate in the Department of Computer Science and Engineering, State University of New York, Buffalo in 2012, and then in the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA. Dr. Kong's research in Computer Vision and Machine Learning is supported by National Science Foundation, Army Research Office, and Office of Naval Research, etc. His work has been publishing on top-tier conferences and transactions in the AI community such as CVPR, ECCV, ICCV, T-PAMI, IJCV, etc. He is an Associate Editor for Springer Journal of Multimedia Systems, and also serves as reviewers and PC members for prestige journals and conferences, including T-PAMI, T-IP, T-NNLS, T-CSVT, CVPR, ICLR, AAAI, and IJCAI, etc. More information can be found on his webpage at https://people.rit.edu/yukics/. 
Yu Kong, Rochester Institute of Technology WebEx - link to be emailed ahead of seminar   10:00 am    
 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 
  
    

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


 

Feb
1
2022




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


Feb
1
2022
Feb12022


Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.



Oshani Seneviratne, Rensselaer Polytechnic Institute 
WebEx - link to be emailed ahead of seminar   10:00 am


Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.




Challenges and Opportunities in Decentralized Systems
 
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.

Challenges and Opportunities in Decentralized Systems
 From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails.
 
Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.
From the invention of the Web three decades ago to the formulation of blockchain technologies only a little over a decade ago, decentralized information systems have transformed (and continue to transform) various aspects of human experience from healthcare, finance, education, entertainment, research, and social connections. In this talk, I will outline several challenges and opportunities in decentralized systems and how I have addressed and leveraged them in my research. First, I will discuss a study on policy-aware content reuse on the Web and the Hyper-Text Transfer Protocol with Accountability (HTTPA) that I developed as part of my Ph.D. dissertation. HTTPA enables individuals to share, reuse and repurpose web resources in a policy preserving manner and resource owners to investigate how their resources were used. Next, I will discuss methods to handle "break-glass in case of emergency" situations in blockchain-based decentralized applications using novel smart contract execution frameworks. For this purpose, I have investigated methods for strengthening smart contracts with novel consensus mechanisms, swarm contracts for heterogeneous agent environments, and fast and scalable computation mechanisms for data-heavy decentralized applications. I will conclude the talk with an overview of my future research agenda that aims to combine web and blockchain technologies in novel ways to address nascent challenges and opportunities. Examples include handling (crypto-)misinformation in social media, developing sustainable data-sharing platforms with incentives, and interpretable data-centric AI infrastructures that are fortified with blockchain-based audit trails. Oshani Seneviratne is the Director of Health Data Research at the Institute for Data Exploration and Application at Rensselaer Polytechnic Institute. Oshani obtained her Ph.D. in Computer Science from the Massachusetts Institute of Technology (MIT) under the supervision of Sir Tim Berners-Lee, the inventor of the World Wide Web. Oshani's research interests span decentralized systems that include web and blockchain technologies, data integration, knowledge graphs, artificial intelligence, and health systems. Oshani has published over 50 articles in these research areas and won the Yahoo! Key Scientific Challenge Award for her dissertation work. She has co-founded and co-organized the AIChain workshop series co-located with the IEEE Blockchain conference, the Personal Health Knowledge Graph workshop series at the Knowledge Graph Conference, and the AAAI Symposium on AI for Social Good. Oshani has served in organizing committees of the International Semantic Web Conference, Web Science Conference, and IEEE Blockchain Conference. Oshani is currently a co-editor of the Semantic Technologies for Data and Algorithmic Governance issue at the Semantic Web Journal and the Personal Health special issue at the Data Intelligence journal. She is also an active reviewer for journals such as Web Semantics, Medical Internet Research, Biomedical and Health Informatics, and several conferences specializing in web technologies, web science, semantic web, knowledge graphs, blockchain, and health systems.Oshani Seneviratne, Rensselaer Polytechnic Institute WebEx - link to be emailed ahead of seminar   10:00 am    
 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 
  
    

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


 

Jan
20
2022




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


Jan
20
2022
Jan202022


Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.



Daniel Brown, UC Berkeley 
WebEx - link to be emailed ahead of seminar   11:00 am


Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.




Leveraging Human Input to Enable Robust AI Systems
Abstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.

Leveraging Human Input to Enable Robust AI SystemsAbstract:In this talk I will discuss recent progress towards using human input to enable safe and robust autonomous systems. Whereas much work on robust machine learning and control seeks to be resilient to or remove the need for human input, my research seeks to directly and efficiently incorporate human input into the study of robust AI systems. One problem that arises when robots and other AI systems learn from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, I will discuss prior and ongoing research along three main topics: (1) how to enable AI systems to efficiently and accurately maintain uncertainty over human intent, (2) how to generate risk-averse behaviors that are robust to this uncertainty, and (3) how robots and other AI systems can efficiently query for additional human input to actively reduce uncertainty and improve their performance. My talk will conclude with a discussion of my long-term vision for safe and robust autonomy, including learning from multi-modal human input, interpretable and verifiable robustness, and developing techniques for human-in-the-loop robust machine learning that generalize beyond reward function uncertainty.Bio:Daniel Brown is a postdoctoral scholar at UC Berkeley, advised by Anca Dragan and Ken Goldberg. His research focuses on safe and robust autonomous systems, with an emphasis on robot learning under uncertainty, human-AI interaction, and value alignment of AI systems. He evaluates his research across a range of applications, including autonomous driving, service robotics, and dexterous manipulation. Daniel received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe and efficient inverse reinforcement learning. Prior to starting his PhD, Daniel was a research scientist at the Air Force Research Lab's Information Directorate where he studied bio-inspired swarms and multi-agent systems. Daniel’s research has been nominated for two best-paper awards and he was selected in 2021 as a Robotics: Science and Systems Pioneer.
Daniel Brown, UC Berkeley WebEx - link to be emailed ahead of seminar   11:00 am    


EventsComputer Science Colloquium
Competitions and Hackathons
PhD Thesis Defenses



EventsComputer Science Colloquium
Competitions and Hackathons
PhD Thesis Defenses





 Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
  





 Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
  



 Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
  

 Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
   Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
 Contact Information
Computer Science
207 Lally Hall110 8th StreetTroy, NY 12180-3590
(518) 276 8412
Detailed Contact Information
Detailed Contact Information






Rensselaer Polytechnic Institute


              110 Eighth Street |
              Troy, NY USA 12180
            

(518) 276-6000

Contact Us







Policies:

      Media

      Web Privacy

      Title IX


    Student Consumer Information

    Accessibility
 
            Copyright © 2019 Rensselaer Polytechnic Institute (RPI)
          










Rensselaer Polytechnic Institute


              110 Eighth Street |
              Troy, NY USA 12180
            

(518) 276-6000

Contact Us







Policies:

      Media

      Web Privacy

      Title IX


    Student Consumer Information

    Accessibility
 
            Copyright © 2019 Rensselaer Polytechnic Institute (RPI)
          







Rensselaer Polytechnic Institute


              110 Eighth Street |
              Troy, NY USA 12180
            

(518) 276-6000

Contact Us







Policies:

      Media

      Web Privacy

      Title IX


    Student Consumer Information

    Accessibility
 
            Copyright © 2019 Rensselaer Polytechnic Institute (RPI)
          





Rensselaer Polytechnic Institute


              110 Eighth Street |
              Troy, NY USA 12180
            

(518) 276-6000

Contact Us


Rensselaer Polytechnic Institute

              110 Eighth Street |
              Troy, NY USA 12180
            

              110 Eighth Street |
              Troy, NY USA 12180
            (518) 276-6000
Contact Us




Policies:

      Media

      Web Privacy

      Title IX


    Student Consumer Information

    Accessibility
 
            Copyright © 2019 Rensselaer Polytechnic Institute (RPI)
          




Policies:

      Media

      Web Privacy

      Title IX


    Student Consumer Information

    Accessibility
 
            Copyright © 2019 Rensselaer Polytechnic Institute (RPI)
          

Policies:

      Media

      Web Privacy

      Title IX


















